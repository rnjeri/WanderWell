{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "import os               # for environ variables in Part 3\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### this is for converting a silly pandas format to something that is more like a normal json you may want it later\n",
    "\n",
    "# panda = items.toPandas()\n",
    "\n",
    "# pcols = panda.columns\n",
    "\n",
    "# pcols = cycle(pcols)\n",
    "\n",
    "# jint = 0\n",
    "# jlist = []\n",
    "# jdict = {}\n",
    "\n",
    "# for i in panda.values.flatten():\n",
    "#     key = pcols.next()\n",
    "#     jdict[key] = i\n",
    "#     if key == 'user_id':\n",
    "#         jlist.append(jdict)\n",
    "#         jdict = {}\n",
    "#         jint += 1\n",
    "   \n",
    "\n",
    "\n",
    "# with open('../data/acc_dataset_local.json', 'w') as jfile:\n",
    "#     json.dump(jlist, jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yelp = spark.read.json('../data/yelp_academic_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp = spark.read.json('../data/acc_dataset_local.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kill_non_ascii(text):\n",
    "    lets = []\n",
    "    ntext = text.lower()\n",
    "    ntext = re.sub(\"[^a-z' ]\",' ',ntext)\n",
    "#     for letter in ntext:\n",
    "#         if ord(letter) < 128 :\n",
    "#             lets.append(letter.lower())\n",
    "    return ntext.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.udf.register('sbin', lambda x: 1 if x > 3 else 0)\n",
    "spark.udf.register('imbin', lambda x: 1 if x > 2 else 0)\n",
    "spark.udf.register('mkascii', kill_non_ascii)\n",
    "spark.udf.register('listjoin', lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp.registerTempTable('yelp')\n",
    "\n",
    "r_bin =  spark.sql('''\n",
    "            SELECT array(mkascii(text)) as content, int(imbin(useful + funny + cool)) as relevant, int(sbin(stars)) as good\n",
    "            FROM yelp\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(content=[u'[saturday, night, late, i, was, getting, warm, when, i, checked, the, thermostat, to, see, if, the, central, ac, was, on, and, yes, it, was, but, it, was, blowing, warm, air, oh, no, so, now, my, air, conditioning, decided, the, day, it, was, degrees, to, stop, working, i, called, sunday, afternoon, and, spoke, with, mark, i, told, him, about, the, issue, with, my, ac, and, he, said, the, earliest, he, could, get, here, was, sometime, monday, i, was, fine, with, that, even, tough, it, was, degrees, in, the, house, my, wife, and, i, were, ok, but, a, bit, worried, about, our, dogs, and, how, they, would, take, the, heat, amy, marks, wife, called, this, morning, to, confirm, that, i, would, be, home, mark, came, around, asked, a, few, questions, and, went, right, to, work, after, diagnosing, the, problem, he, came, back, and, told, us, what, was, wrong, what, needed, to, be, done, and, the, cost, to, have, it, repaired, i, agreed, to, the, repair, about, hour, later, he, was, done, i, believe, he, was, here, for, a, total, of, thirty, minutes, and, my, central, air, conditioning, unit, is, now, working, and, blowing, cold, air, happy, dance, he, located, tested, and, fixed, the, problem, bad, capacitor, quickly, and, efficiently, i, can, sit, back, now, and, relax, in, my, fully, air, conditioned, house, and, enjoy, the, cool, air, as, i, write, this, review, price, no, hidden, or, extra, fees, for, an, estimate, just, the, cost, of, the, repair, service, mark, is, very, professional, overall, this, company, was, as, good, as, you, could, ask, for, conclusion, they, were, prompt, thorough, and, affordable, and, highly, recommend, in, fact, yelp, readers, look, no, further, this, is, a, quality, ethical, reasonably, priced, a, c, service, could, not, be, happier]'], relevant=1, good=1)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_bin.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mincount =  r_bin.filter('relevant > 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_neg = r_bin.filter('relevant = 0').orderBy(rand()).limit(mincount)\n",
    "dataset_pos = r_bin.filter('relevant = 1').orderBy(rand()).limit(mincount)\n",
    "\n",
    "df_relevance = dataset_pos.union(dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_relevance = df_relevance.drop('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- relevant: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_relevance.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"content\", outputCol=\"filtered\")\n",
    "df_rel_stopped = remover.transform(df_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- relevant: integer (nullable = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rel_stopped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(df_rel_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- relevant: integer (nullable = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rel_stopped.registerTempTable('df_rel_stopped')\n",
    "\n",
    "stop_strings = spark.sql('''\n",
    "            SELECT listjoin(filtered) as filtered, content, relevant\n",
    "            FROM df_rel_stopped\n",
    "            ''')\n",
    "\n",
    "### add ngrams later if needed\n",
    "# ngramDataFrame.registerTempTable('ngrammed_stopped_rel')\n",
    "\n",
    "# stop_strings = spark.sql('''\n",
    "#             SELECT listjoin(filtered) as filtered, ngrams, content, relevant\n",
    "#             FROM ngrammed_stopped_rel\n",
    "#             ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"filtered\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(stop_strings)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2500)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered: string (nullable = true)\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- relevant: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(2500, {61: 3.2888, 117: 1.9087, 132: 1.286, 224: 1.8984, 232: 2.193, 297: 0.4224, 405: 2.6723, 614: 1.1202, 666: 1.6664, 696: 0.8122, 813: 4.201, 855: 1.2046, 906: 1.1156, 1103: 0.9572, 1142: 4.3087, 1171: 3.3924, 1183: 3.662, 1184: 0.2636, 1207: 2.9454, 1220: 1.1831, 1309: 2.765, 1388: 2.9338, 1468: 0.5055, 1658: 4.9574, 1663: 4.6917, 1685: 0.3915, 1768: 1.6893, 1848: 0.8515, 1863: 2.3563, 1873: 2.0015, 1946: 0.6801, 2092: 1.977, 2096: 2.6723, 2139: 3.6502, 2179: 1.2547, 2200: 0.3792, 2299: 2.713, 2304: 2.258, 2432: 9.5186, 2459: 4.0675, 2473: 2.8406}))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.printSchema()\n",
    "\n",
    "rescaledData.select('features').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autostem(cell):\n",
    "    return cell.asDict()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(count(text)=4153150)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('pstem', autostem )\n",
    "\n",
    "all_corpus = spark.sql('''\n",
    "                    SELECT count(text)\n",
    "                    FROM yelp\n",
    "                    '''\n",
    "                    )\n",
    "\n",
    "all_corpus.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.sparktools import SparkNLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SparkNLPClassifier('../data/acc_dataset_local.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AttributeError: \"'GBTClassifier' object has no attribute '_java_obj'\" in <object repr() failed> ignored\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-384-c767c79b9502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'useful + funny + cool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/travis/Documents/Education/Galvanize/WanderWell/src/sparktools.py\u001b[0m in \u001b[0;36mtrain_vectorize\u001b[0;34m(self, thres, n_features)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mApplies\u001b[0m \u001b[0mvectorize\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         '''\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_binary_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_labeled_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "model.train_vectorize('useful + funny + cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = model.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered: string (nullable = true)\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'probabilityCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-402-9897ccdd9b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_boosted_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/travis/Documents/Education/Galvanize/WanderWell/src/sparktools.py\u001b[0m in \u001b[0;36mtrain_boosted_forest\u001b[0;34m(self, depth, n_trees, learning_rate)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         gbr = GBTClassifier(labelCol='label', featuresCol=\"features\", probabilityCol=\"probability\",\n\u001b[0;32m--> 176\u001b[0;31m                              maxDepth=depth, maxIter=n_trees, stepSize=learning_rate)\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatureIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Train model.  This also runs the indexer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/__init__.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'probabilityCol'"
     ]
    }
   ],
   "source": [
    "model.train_boosted_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered: string (nullable = true)\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- indexedFeatures: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       0.0|\n",
      "|    1|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('label', 'prediction').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TRAINING RANDOM FOREST\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "# this is done\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            filtered|             content|label|               words|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|['d, just, like, ...|[['d, just, like,...|    1|[['d,, just,, lik...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-258.27869605749...|[0.49853738144182...|       1.0|\n",
      "|[a, fan, for, sur...|[[a, fan, for, su...|    1|[[a,, fan,, for,,...|(25,[0,2,3,4,5,6,...|(25,[0,2,3,4,5,6,...|[-38.758179370807...|[0.49716334288939...|       1.0|\n",
      "|[a, friend, of, a...|[[a, friend, of, ...|    1|[[a,, friend,, of...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-94.157227336214...|[0.48563119313127...|       1.0|\n",
      "|[a, friend, of, o...|[[a, friend, of, ...|    1|[[a,, friend,, of...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-83.746823626728...|[0.50287976329242...|       0.0|\n",
      "|[a, gloriously, s...|[[a, gloriously, ...|    1|[[a,, gloriously,...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-108.13114797253...|[0.50036056382243...|       0.0|\n",
      "|[a, good, pair, o...|[[a, good, pair, ...|    1|[[a,, good,, pair...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-30.498938087595...|[0.50442559455620...|       0.0|\n",
      "|[a, great, place,...|[[a, great, place...|    1|[[a,, great,, pla...|(25,[0,2,3,4,5,6,...|(25,[0,2,3,4,5,6,...|[-16.429837956033...|[0.49490269467951...|       1.0|\n",
      "|[a, group, of, us...|[[a, group, of, u...|    1|[[a,, group,, of,...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-57.344374195563...|[0.54383020891491...|       0.0|\n",
      "|[a, popular, stop...|[[a, popular, sto...|    1|[[a,, popular,, s...|(25,[0,1,2,4,5,6,...|(25,[0,1,2,4,5,6,...|[-29.467031794352...|[0.49060389503051...|       1.0|\n",
      "|[a, total, dive, ...|[[a, total, dive,...|    1|[[a,, total,, div...|(25,[0,2,3,4,5,6,...|(25,[0,2,3,4,5,6,...|[-7.1057668108310...|[0.50262870099478...|       0.0|\n",
      "|[a, vegas, staple...|[[a, vegas, stapl...|    1|[[a,, vegas,, sta...|(25,[0,2,3,4,5,6,...|(25,[0,2,3,4,5,6,...|[-24.018459688624...|[0.49209348417157...|       1.0|\n",
      "|[a, very, complex...|[[a, very, comple...|    1|[[a,, very,, comp...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-36.456771102304...|[0.51990531109839...|       0.0|\n",
      "|[absolutely, amaz...|[[absolutely, ama...|    1|[[absolutely,, am...|(25,[0,2,4,5,6,8,...|(25,[0,2,4,5,6,8,...|[-14.274473689194...|[0.49972646185931...|       1.0|\n",
      "|[absolutely, the,...|[[absolutely, the...|    1|[[absolutely,, th...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-12.817936880953...|[0.50066579164836...|       0.0|\n",
      "|[after, attemptin...|[[after, attempti...|    1|[[after,, attempt...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-85.848291000031...|[0.48625615583014...|       1.0|\n",
      "|[after, executing...|[[after, executin...|    1|[[after,, executi...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-31.935553889451...|[0.50638092995541...|       0.0|\n",
      "|[after, having, a...|[[after, having, ...|    1|[[after,, having,...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-49.248930095914...|[0.51213355062050...|       0.0|\n",
      "|[after, years, of...|[[after, years, o...|    1|[[after,, years,,...|(25,[0,2,3,4,5,8,...|(25,[0,2,3,4,5,8,...|[-32.208957700561...|[0.53420540283283...|       0.0|\n",
      "|[again, i, need, ...|[[again, i, need,...|    1|[[again,, i,, nee...|(25,[0,1,2,5,8,9,...|(25,[0,1,2,5,8,9,...|[-17.289369341263...|[0.50355332451751...|       0.0|\n",
      "|[ah, the, buffet,...|[[ah, the, buffet...|    1|[[ah,, the,, buff...|(25,[0,1,2,3,4,5,...|(25,[0,1,2,3,4,5,...|[-81.635326802950...|[0.50144029161217...|       0.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TRAIN NAIVE BAYES\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(model.train_popular)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(mpos)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered: string (nullable = true)\n",
      " |-- content: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       1.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       0.0|    1|\n",
      "|       1.0|    1|\n",
      "|       1.0|    1|\n",
      "+----------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sortby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-600e718af7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sortby'"
     ]
    }
   ],
   "source": [
    "prediction.select('prediction', 'label').show(30).sortby('probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction.registerTempTable('pred')\n",
    "spark.udf.register('acc', lambda x,y: 1 if x==y else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`probability`' given input columns: [accur]; line 4 pos 17;\\n'Sort ['probability DESC NULLS LAST], true\\n+- Project [acc(prediction#4006, label#3539) AS accur#4143]\\n   +- SubqueryAlias pred\\n      +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, features#3635, indexedFeatures#3997, UDF(features#3635) AS prediction#4006]\\n         +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, features#3635, UDF(features#3635) AS indexedFeatures#3997]\\n            +- Sample 0.8, 1.0, false, 202007017856859376\\n               +- Sort [filtered#3588 ASC NULLS FIRST, content#3538 ASC NULLS FIRST, label#3539 ASC NULLS FIRST, words#3593 ASC NULLS FIRST, rawFeatures#3599 ASC NULLS FIRST, features#3635 ASC NULLS FIRST], false\\n                  +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, UDF(rawFeatures#3599) AS features#3635]\\n                     +- Project [filtered#3588, content#3538, label#3539, words#3593, UDF(words#3593) AS rawFeatures#3599]\\n                        +- Project [filtered#3588, content#3538, label#3539, UDF(filtered#3588) AS words#3593]\\n                           +- Project [listjoin(filtered#3582) AS filtered#3588, content#3538, label#3539]\\n                              +- SubqueryAlias df_lab_stopped\\n                                 +- Project [content#3538, label#3539, UDF(content#3538) AS filtered#3582]\\n                                    +- Union\\n                                       :- GlobalLimit 3858\\n                                       :  +- LocalLimit 3858\\n                                       :     +- Project [content#3538, label#3539]\\n                                       :        +- Sort [_nondeterministic#3576 ASC NULLS FIRST], true\\n                                       :           +- Project [content#3538, label#3539, rand(-3305983844601206200) AS _nondeterministic#3576]\\n                                       :              +- Filter (label#3539 = 1)\\n                                       :                 +- Project [array(words_only(text#3522)) AS content#3538, cast(cast(imbin(((useful#3524L + funny#3519L) + cool#3517L)) as decimal(20,0)) as int) AS label#3539]\\n                                       :                    +- SubqueryAlias df\\n                                       :                       +- Relation[business_id#3516,cool#3517L,date#3518,funny#3519L,review_id#3520,stars#3521L,text#3522,type#3523,useful#3524L,user_id#3525] json\\n                                       +- GlobalLimit 3858\\n                                          +- LocalLimit 3858\\n                                             +- Project [content#3538, label#3539]\\n                                                +- Sort [_nondeterministic#3572 ASC NULLS FIRST], true\\n                                                   +- Project [content#3538, label#3539, rand(3810529930739689851) AS _nondeterministic#3572]\\n                                                      +- Filter (label#3539 = 0)\\n                                                         +- Project [array(words_only(text#3522)) AS content#3538, cast(cast(imbin(((useful#3524L + funny#3519L) + cool#3517L)) as decimal(20,0)) as int) AS label#3539]\\n                                                            +- SubqueryAlias df\\n                                                               +- Relation[business_id#3516,cool#3517L,date#3518,funny#3519L,review_id#3520,stars#3521L,text#3522,type#3523,useful#3524L,user_id#3525] json\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-399-7c57e83cbafd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mFROM\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mDESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m ''')\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`probability`' given input columns: [accur]; line 4 pos 17;\\n'Sort ['probability DESC NULLS LAST], true\\n+- Project [acc(prediction#4006, label#3539) AS accur#4143]\\n   +- SubqueryAlias pred\\n      +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, features#3635, indexedFeatures#3997, UDF(features#3635) AS prediction#4006]\\n         +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, features#3635, UDF(features#3635) AS indexedFeatures#3997]\\n            +- Sample 0.8, 1.0, false, 202007017856859376\\n               +- Sort [filtered#3588 ASC NULLS FIRST, content#3538 ASC NULLS FIRST, label#3539 ASC NULLS FIRST, words#3593 ASC NULLS FIRST, rawFeatures#3599 ASC NULLS FIRST, features#3635 ASC NULLS FIRST], false\\n                  +- Project [filtered#3588, content#3538, label#3539, words#3593, rawFeatures#3599, UDF(rawFeatures#3599) AS features#3635]\\n                     +- Project [filtered#3588, content#3538, label#3539, words#3593, UDF(words#3593) AS rawFeatures#3599]\\n                        +- Project [filtered#3588, content#3538, label#3539, UDF(filtered#3588) AS words#3593]\\n                           +- Project [listjoin(filtered#3582) AS filtered#3588, content#3538, label#3539]\\n                              +- SubqueryAlias df_lab_stopped\\n                                 +- Project [content#3538, label#3539, UDF(content#3538) AS filtered#3582]\\n                                    +- Union\\n                                       :- GlobalLimit 3858\\n                                       :  +- LocalLimit 3858\\n                                       :     +- Project [content#3538, label#3539]\\n                                       :        +- Sort [_nondeterministic#3576 ASC NULLS FIRST], true\\n                                       :           +- Project [content#3538, label#3539, rand(-3305983844601206200) AS _nondeterministic#3576]\\n                                       :              +- Filter (label#3539 = 1)\\n                                       :                 +- Project [array(words_only(text#3522)) AS content#3538, cast(cast(imbin(((useful#3524L + funny#3519L) + cool#3517L)) as decimal(20,0)) as int) AS label#3539]\\n                                       :                    +- SubqueryAlias df\\n                                       :                       +- Relation[business_id#3516,cool#3517L,date#3518,funny#3519L,review_id#3520,stars#3521L,text#3522,type#3523,useful#3524L,user_id#3525] json\\n                                       +- GlobalLimit 3858\\n                                          +- LocalLimit 3858\\n                                             +- Project [content#3538, label#3539]\\n                                                +- Sort [_nondeterministic#3572 ASC NULLS FIRST], true\\n                                                   +- Project [content#3538, label#3539, rand(3810529930739689851) AS _nondeterministic#3572]\\n                                                      +- Filter (label#3539 = 0)\\n                                                         +- Project [array(words_only(text#3522)) AS content#3538, cast(cast(imbin(((useful#3524L + funny#3519L) + cool#3517L)) as decimal(20,0)) as int) AS label#3539]\\n                                                            +- SubqueryAlias df\\n                                                               +- Relation[business_id#3516,cool#3517L,date#3518,funny#3519L,review_id#3520,stars#3521L,text#3522,type#3523,useful#3524L,user_id#3525] json\\n\""
     ]
    }
   ],
   "source": [
    "accs = spark.sql('''\n",
    "        SELECT acc(prediction, label) as accur\n",
    "        FROM pred\n",
    "        ORDER BY probability DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
